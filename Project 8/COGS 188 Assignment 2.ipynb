{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u> COGS 188 Assignment 2 </u>\n",
    "\n",
    "In this Assignment we will be implementing both **K-means clustering** and **EM Algorithm** on 2D points. Both of these algorithms are methods to divide the given set of points into **k** clusters (groups), such that all points in a cluster are \"close together\". The number **k** is chosen in advance manually and is not computed by the algorithms.\n",
    "\n",
    "The main difference between the two algorithms is that K-means is a \"hard clustering\" method, whereas EM is used to create a \"soft clustering\". This means that in K-means, each point is either 100% *in* a cluster, or not in it. Whereas in EM Algorithm, we compute a (Gaussian) probability distribution for each cluster. We can use these to compute the probability for a point to be in a cluster, for all pairs of points and clusters. So for each point, we find the probability that it is in each of the clusters.\n",
    "\n",
    "***In this assignment, please complete the code only in the cells which mention that you should do so in the code comments.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1 - Kmeans\n",
    "\n",
    "K-means clustering is a type of unsupervised learning, which is used with unlabeled dataset. The goal of this algorithm is to find K groups in the data. The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. Data points are clustered based on feature similarity. The results of the K-means clustering algorithm are:\n",
    "\n",
    "- The centroids of the K clusters, which can be used to label new data\n",
    "- Labels for the training data (each data point is assigned to a single cluster)\n",
    "\n",
    "K-means works by defining spherical clusters that are separable in a way so that the mean value converges towards the cluster center. Because of this, K-Means may underperform sometimes.\n",
    "\n",
    "<b><u>Use Cases:</u></b>\n",
    "- Document Classification\n",
    "- Delivery Store Optimization\n",
    "- Customer Segmentation\n",
    "- Insurance Fraud Detection etc.\n",
    "\n",
    "### <u> Algorithm </u>:\n",
    "\n",
    "Κ-means clustering algorithm inputs are the number of clusters Κ and the data set. Algorithm starts with initial estimates for the Κ centroids, which can either be randomly generated or randomly selected from the data set. The algorithm then iterates between two steps:\n",
    "\n",
    "<b><u>1. Data assigment step:</u></b>\n",
    "\n",
    "Each centroid defines one of the clusters. In this step, each data point based on the squared Euclidean distance is assigned to its nearest centroid. If $c_i$ is the collection of centroids in set C, then each data point x is assigned to a cluster based on\n",
    "\n",
    "$$\\underset{c_i \\in C}{\\min} \\; dist(c_i,x)^2$$\n",
    "\n",
    "where dist( · ) is the standard (L2) Euclidean distance.\n",
    "\n",
    "<b><u>2. Centroid update step:</u></b>\n",
    "\n",
    "Centroids are recomputed by taking the mean of all data points assigned to that centroid's cluster.\n",
    "\n",
    "The algorithm iterates between step one and two until a stopping criteria is met (no data points change clusters, the sum of the distances is minimized, or some maximum number of iterations is reached). In this assignment, we will use the first criterion, i.e. that no cluster centers change.\n",
    "\n",
    "<b>This algorithm may converge on a local optimum. </b> Assessing more than one run of the algorithm with randomized starting centroids may give a better outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the data\n",
    "\n",
    "We will generate 3 datasets to use in this assignment. They are created in the following code, and stored in `X`, `X1` and `X2` respectively. They are each sets of 500 2D points. We store them as matrices of size $500 \\times 2$.\n",
    "\n",
    "Please use these variables `X1`, `X2` and `X3` to run your kmeans and EM in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell has been completed for you. You don't need to change it.\n",
    "\"\"\"\n",
    "\n",
    "X,Y = make_blobs(cluster_std=1.5,random_state=20,n_samples=500,centers=3)\n",
    "X = np.dot(X,np.random.RandomState(0).randn(2,2))\n",
    "\n",
    "print('X shape: ', X.shape)\n",
    "\n",
    "plt.scatter([x[0] for x in X], [x[1] for x in X])\n",
    "plt.title('X')\n",
    "plt.show()\n",
    "\n",
    "centers = [[4, 7], [9, 9], [9, 2]]\n",
    "\n",
    "X1,Y1 = make_blobs(cluster_std=1.5,random_state=20,n_samples=500,centers=centers)\n",
    "X1 = np.dot(X1,np.random.RandomState(0).randn(2,2))\n",
    "\n",
    "print('X1 shape: ', X1.shape)\n",
    "\n",
    "plt.scatter([x[0] for x in X1], [x[1] for x in X1])\n",
    "plt.title('X1')\n",
    "plt.show()\n",
    "\n",
    "centers = [[5, 5]]\n",
    "X21,Y21 = make_blobs(cluster_std=1.5,random_state=20,n_samples=200,centers=centers)\n",
    "X21 = np.dot(X21, np.array([[1.0, 0], [0, 5.0]]))\n",
    "\n",
    "centers = [[5, 5]]\n",
    "X22,Y22 = make_blobs(cluster_std=1.5,random_state=20,n_samples=200,centers=centers)\n",
    "X22 = np.dot(X21, np.array([[5.0, 0], [0, 1.0]]))\n",
    "\n",
    "centers = [[7, 7]]\n",
    "X23, Y23 = make_blobs(cluster_std=1.5,random_state=20,n_samples=100,centers=centers)\n",
    "X23 = np.dot(X23, np.random.RandomState(0).randn(2,2))\n",
    "\n",
    "X2 = np.vstack((X21, X22, X23))\n",
    "\n",
    "print('X2 shape: ', X2.shape)\n",
    "\n",
    "plt.scatter([x[0] for x in X2], [x[1] for x in X2])\n",
    "plt.title('X2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans Algorithm Class\n",
    "\n",
    "Now we will start coding the K-means algorithm. All the code will be in the `KmeansModel` class.\n",
    "\n",
    "We start by initializing the class. To initialize, we specify the data matrix **X** and the number of clusters **k**. **X** is a matrix of shape $N \\times 2$, as we have **N** points each with x and y coordinates. We set `self.N` as the number of points and `self.dim` as the number of dimensions (2).\n",
    "\n",
    "We need to store $k$ centroids, one for each cluster. We shall store them as a $K \\times 2$ matrix in `self.centroids`. We initialize the centroids to be random points from our dataset.\n",
    "\n",
    "If the model is not converging, we stop running it if the number of iterations has reached `max_iters`.\n",
    "\n",
    "Finally we plot the data with the initial (random) clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KmeansModel:\n",
    "    \n",
    "    def __init__(self, X, k, max_iters):\n",
    "        self.X = X\n",
    "        self.k = k\n",
    "        self.max_iters = max_iters\n",
    "        \n",
    "        self.dim = X.shape[1]\n",
    "        self.N = X.shape[0]\n",
    "        \n",
    "        centroid_indices = np.random.RandomState(2).permutation(X.shape[0])[:k]\n",
    "        self.centroids = X[centroid_indices]\n",
    "        \n",
    "        initial_labels = self.get_labels(self.X, self.centroids)\n",
    "        self.plot_data(initial_labels, 'Data with initial random clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To color each point according to its cluster, we need to specify a **label** for each point. A label is just an integer `>= 0`. The label for all points in the same cluster must be the same.\n",
    "\n",
    "So we make a function `get_labels(X, centroids)`. This function takes in our $N \\times 2$ data matrix **X**, and our $k \\times 2$ centroid matrix. It should return a $N \\times 1$ numpy array `labels`, such that `labels[i]` is the index of the cluster of the `i`'th point.\n",
    "\n",
    "To do this, we should iterate over all the points. For each point we should iterate over all the centroids, and choose the one which is the closest (least Euclidean distance) to the point. We can simply use the index of that centroid (`0 <= index < k`) as the label for that point. We should finally return the array of labels for all the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(self, X, centroids):\n",
    "    \n",
    "    # REPLACE THIS WITH THE PROPER CODE\n",
    "    labels = [0 for i in range(self.N)]\n",
    "\n",
    "    return np.array(labels)\n",
    "\n",
    "KmeansModel.get_labels = get_labels # Here we are just adding the function to the KmeansModel class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a function to plot our data. This function takes in the labels, and colors each point according to its cluster, as defined by the labels we computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(self, labels, title):\n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    ax0 = fig.add_subplot(111)\n",
    "    ax0.scatter(self.X[:,0], self.X[:,1], c=labels)\n",
    "    ax0.set_title(title)\n",
    "    \n",
    "KmeansModel.plot_data = plot_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our main function which runs the K-means algorithm. This repeatedly performs the two steps of the Kmeans algorithm (Data Assignment and Centroid Update). You should stop when there is no change in the cluster centroids, ie they are the same as the previous step. \n",
    "\n",
    "We also stop after a certain number of max iterations if the algorithm is not converging.\n",
    "\n",
    "After convergence, we plot the data again with the new cluster centers and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(self):\n",
    "    iters = 0\n",
    "    while True:\n",
    "        # REPLACE THIS WITH THE ACTUAL NEW CENTROIDS CALCULATION. USE get_labels()\n",
    "        new_centroids = centroids\n",
    "\n",
    "        # ADD THE CONVERGENCE CONDITION HERE, IE IF THE CENTROIDS DON'T CHANGE SINCE THE LAST ITERATION THEN BREAK\n",
    "        # ALSO UPDATE THE CENTROIDS TO THE NEW VALUES\n",
    "            \n",
    "        if iters == self.max_iters:\n",
    "            break\n",
    "        iters += 1\n",
    "\n",
    "    final_labels = self.get_labels(self.X, self.centroids)\n",
    "    self.plot_data(final_labels, 'Final clusters')\n",
    "    \n",
    "KmeansModel.run = run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create an instance of `KmeansModel` and run our algorithm on some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km1 = KmeansModel(X, 3, 100)\n",
    "km1.run()\n",
    "\n",
    "km2 = KmeansModel(X1, 3, 100)\n",
    "km2.run()\n",
    "\n",
    "km3 = KmeansModel(X2, 3, 100)\n",
    "km3.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 - EM\n",
    "\n",
    "EM Algorithm finds clusters by treating each cluster as a 2D Gaussian distribution, and then fits the parameters of each distribution to accurately model the data. The goal is to find optimal parameters for each of the Gaussians which model the data accurately.\n",
    "\n",
    "<img src=\"em1.PNG\">\n",
    "\n",
    "Each cluster distribution is of the form $\\pi_c \\mathcal{N}(\\mu_c, \\sum_c)$, for each cluster $c, 0 \\leq c < k$. For each cluster, we have the parameters $\\mu_c, \\pi_c$ and $\\sum_c$.\n",
    "\n",
    "$\\pi_c$ denotes the cluster \"strength\" or \"size\" for each cluster. It represents the prior strength for a cluster, i.e. how likely it is that any point in the dataset belongs to that cluster.\n",
    "\n",
    "$\\mu_c$ denotes the cluster mean (a 2D point). The cluster distribution is centered around this point.\n",
    "\n",
    "$\\sum_c$ denotes the cluster covariance. Covariance is a generalization of variance $\\sigma$ to multiple variables (in our case, 2D coordinates). It is a $2 \\times 2$ matrix which represents the spread of each of the $x$ and $y$ coordinates.\n",
    "\n",
    "The EM Algorithm has 2 steps, which are repeated till convergence:\n",
    "- Expectation: We treat all of the cluster parameters as fixed. We then compute the probabilities of each point belonging to each cluster.\n",
    "- Maximization: We treat the cluster probabilities as fixed and update the distribution parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by initializing our EMModel class. This is the main class which will store our data and distribution parameters. We will also write all the functionality in it.\n",
    "\n",
    "`X` is a matrix of size $N \\times 2$\n",
    "\n",
    "`k` is the number of clusters we want (3 in our case).\n",
    "\n",
    "`max_iters` is the number of iterations we will run the algorithm for.\n",
    "\n",
    "`self.dim` is the number of dimensions (2 in our case).\n",
    "\n",
    "`self.N` is the number of points in the dataset (500 in our case).\n",
    "\n",
    "`self.mu` stores all the cluster means. Size is $k \\times 2$, as we have to store one point for each cluster. It is initialized to random points lying in the dataset.\n",
    "\n",
    "`self.pi` stores all the cluster priors (strengths). Size is $k \\times 1$, as it is just one number for each cluster. It is initialized to $\\frac{1}{k}$ for each cluster.\n",
    "\n",
    "`self.sigma` stores all the cluster covariance matrices. Size is $k \\times 2 \\times 2$, as we have to store a $2 \\times 2$ matrix for each cluster. It is initialized to diagonal matrices with diagonal value $5.0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions\n",
    "\n",
    "To draw a value from a multivariate normal distribution, use the `scipy.stats.multivariate_normal` function. For example, to draw a value from a normal distribution with mean `mu` and covariance matrix `sigma`, you can use the following code:\n",
    "\n",
    "`val = multivariate_normal(mean=mu, cov=sigma).pdf(X)` where `X` is an $N \\times 2$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMModel:\n",
    "    \n",
    "    def __init__(self, X, k, max_iters):\n",
    "        \"\"\"\n",
    "        This function initializes our parameters (mu, pi and sigma) and plots our data points.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.k = k\n",
    "        self.max_iters = max_iters\n",
    "        \n",
    "        self.dim = self.X.shape[1] # Equals 2, as we are considering 2D points\n",
    "        self.N = self.X.shape[0] # Equals the number of points in the dataset\n",
    "        \n",
    "        \"\"\"\n",
    "        Here we initialize mu, pi and sigma. Mu is k X 2, Pi is k X 1, Sigma is k X 2 X 2\n",
    "        \"\"\"\n",
    "        self.mu = np.random.randint(max(min(self.X[:, 0]), min(self.X[:, 1])), min(max(self.X[:, 0]), max(self.X[:, 1])), size=(self.k, self.dim))\n",
    "        self.pi = np.ones(self.k) / self.k\n",
    "        self.sigma = np.zeros((self.k, self.dim, self.dim))\n",
    "        for i in range(self.k):\n",
    "            np.fill_diagonal(self.sigma[i], 5.0)\n",
    "            \n",
    "        \"\"\"\n",
    "        This part is required to plot clusters\n",
    "        \"\"\"\n",
    "        x,y = np.meshgrid(np.sort(self.X[:,0]),np.sort(self.X[:,1]))\n",
    "        self.XY = np.array([x.flatten(),y.flatten()]).T\n",
    "        \n",
    "        \"\"\"\n",
    "        This variable is required in case the covariance matrix turns out to not be positive semidefinite\n",
    "        \"\"\"\n",
    "        self.sigma_correction = 0.0*np.identity(self.dim)\n",
    "            \n",
    "        \"\"\"\n",
    "        Finally, let's visualize our data points\n",
    "        \"\"\"\n",
    "        self.plot_data('Initial State')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function `plot_data` to plot our data as well as our clusters. This function plots our data points, and creates a contour plot for each cluster using its `mu` and `sigma` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(self, title, colors=None):\n",
    "    \"\"\"\n",
    "    This function creates a scatter plot of all the data points. It also creates a contour plot of the probability \n",
    "    distributions of each of the clusters (specified by mu, pi and sigma)\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    ax0 = fig.add_subplot(111)\n",
    "    ax0.scatter(self.X[:,0], self.X[:,1], c=colors)\n",
    "    ax0.set_title(title)\n",
    "    for m,c in zip(self.mu,self.sigma):\n",
    "        c += self.sigma_correction\n",
    "        multi_normal = multivariate_normal(mean=m,cov=c)\n",
    "        ax0.contour(np.sort(self.X[:,0]),np.sort(self.X[:,1]),multi_normal.pdf(self.XY).reshape(len(self.X),len(self.X)),colors='black',alpha=0.3)\n",
    "        ax0.scatter(m[0],m[1],c='grey',zorder=10,s=100)\n",
    "        \n",
    "EMModel.plot_data = plot_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our \"expectation\" function. This function's job is to compute $r_{ic}$, which is the probability that point $i$ belongs to cluster $c$ ($0 \\leq i < N$, $0 \\leq c < k$).\n",
    "\n",
    "We will store $r_{ic}$ as a matrix of size $N \\times k$. (as we have to store a probability for point cluster pair).\n",
    "\n",
    "$r_{ic}$ is computed as follows:\n",
    "<img src=\"ric.png\">\n",
    "\n",
    "To compute it, we basically:\n",
    "- compute its probability under cluster model $c$\n",
    "- normalize it to sum to 1 using its probabilities from all the clusters\n",
    "\n",
    "Finally we should return `r` after computing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation(self):\n",
    "    \n",
    "        # REPLACE THIS WITH THE ACTUAL r_ic COMPUTATION\n",
    "        r = np.zeros((self.N, self.k))\n",
    "\n",
    "        return r\n",
    "    \n",
    "EMModel.expectation = expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the \"maximization\" step in the `maximization` function. This function's job is to take the previously computed $r_{ic}$ and update the parameters $\\mu_c, \\sum_c$ of each cluster distribution. This is done as follows:\n",
    "<img src=\"m1.png\">\n",
    "\n",
    "Here, $N = \\sum_{c} N_c$\n",
    "\n",
    "Basically you should re-compute `mu`, `sigma` and `pi` using the previously computed `r` as per the given formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximization(self, r):\n",
    "    self.mu = []\n",
    "    self.sigma = []\n",
    "    self.pi = []\n",
    "\n",
    "    # PLEASE ADD THE ACTUAL PARAMETERS COMPUTATION HERE. SET self.mu, self.sigma and self.pi TO THE CORRECT VALUES\n",
    "        \n",
    "EMModel.maximization = maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write our main `run()` function which runs the EM Algorithm. This performs `max_iter` iterations of expectation and maximization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(self):\n",
    "    r = None\n",
    "    for it in range(self.max_iters):\n",
    "        r = self.expectation()\n",
    "        self.maximization(r)\n",
    "\n",
    "    color_labels = np.argmax(r, axis=1)\n",
    "    self.plot_data('Final State', color_labels)\n",
    "    \n",
    "EMModel.run = run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we instantiate the `EMModel` class and run the algorithm on some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EM = EMModel(X, 3, 50)\n",
    "EM.run()\n",
    "\n",
    "EM1 = EMModel(X1, 3, 100)\n",
    "EM1.run()\n",
    "\n",
    "EM2 = EMModel(X2, 3, 100)\n",
    "EM2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please show the output plots for both Kmeans and EM on all 3 datasets `X` , `X1` and `X2`. \n",
    "\n",
    "## Also feel free to generate your own datasets and run Kmeans and EM on them, maybe you might find some other interesting differences!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
