{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eASNvREtBU9G"
   },
   "source": [
    "## 1. Import Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open 'spark-2.4.0-bin-hadoop2.7.tgz'\n"
     ]
    }
   ],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www-us.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.4.0-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget -q http://www-us.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "duCJfMrnGu00"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\work\\Java\\jre1.8.0_301\"\n",
    "os.environ[\"SPARK_HOME\"] = \"C:\\work\\Spark\\spark-3.1.2-bin-hadoop2.7\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\work\\Spark\\spark-3.1.2-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aGOo805LA-fM"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K7bEtO_fBZmE"
   },
   "source": [
    "## 2. Read and check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py): started\n",
      "  Building wheel for wget (setup.py): finished with status 'done'\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9673 sha256=8f2d3ecfc613504d654c61590d670c71e2c5e0896433e28777dca761f52aeab7\n",
      "  Stored in directory: c:\\users\\navee\\appdata\\local\\pip\\cache\\wheels\\bd\\a8\\c3\\3cf2c14a1837a4e04bd98631724e81f33f462d86a1d895fae0\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install wget  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [............................................................................] 918960 / 918960"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "\n",
    "link_to_data = 'https://github.com/tulip-lab/sit742/raw/master/Assessment/2019/data/bank.csv'\n",
    "DataSet = wget.download(link_to_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FQ8Ts9eZBA-M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       age          job  marital  education default  balance housing loan  \\\n",
      "0       59       admin.  married  secondary      no     2343     yes   no   \n",
      "1       56       admin.  married  secondary      no       45      no   no   \n",
      "2       41   technician  married  secondary      no     1270     yes   no   \n",
      "3       55     services  married  secondary      no     2476     yes   no   \n",
      "4       54       admin.  married   tertiary      no      184      no   no   \n",
      "...    ...          ...      ...        ...     ...      ...     ...  ...   \n",
      "11157   33  blue-collar   single    primary      no        1     yes   no   \n",
      "11158   39     services  married  secondary      no      733      no   no   \n",
      "11159   32   technician   single  secondary      no       29      no   no   \n",
      "11160   43   technician  married  secondary      no        0      no  yes   \n",
      "11161   34   technician  married  secondary      no        0      no   no   \n",
      "\n",
      "        contact  day month  duration  campaign  pdays  previous poutcome  \\\n",
      "0       unknown    5   may      1042         1     -1         0  unknown   \n",
      "1       unknown    5   may      1467         1     -1         0  unknown   \n",
      "2       unknown    5   may      1389         1     -1         0  unknown   \n",
      "3       unknown    5   may       579         1     -1         0  unknown   \n",
      "4       unknown    5   may       673         2     -1         0  unknown   \n",
      "...         ...  ...   ...       ...       ...    ...       ...      ...   \n",
      "11157  cellular   20   apr       257         1     -1         0  unknown   \n",
      "11158   unknown   16   jun        83         4     -1         0  unknown   \n",
      "11159  cellular   19   aug       156         2     -1         0  unknown   \n",
      "11160  cellular    8   may         9         2    172         5  failure   \n",
      "11161  cellular    9   jul       628         1     -1         0  unknown   \n",
      "\n",
      "      deposit  \n",
      "0         yes  \n",
      "1         yes  \n",
      "2         yes  \n",
      "3         yes  \n",
      "4         yes  \n",
      "...       ...  \n",
      "11157      no  \n",
      "11158      no  \n",
      "11159      no  \n",
      "11160      no  \n",
      "11161      no  \n",
      "\n",
      "[11162 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "spark = SparkSession.builder.appName('ml-bank').getOrCreate()\n",
    "\n",
    "# Import the 'bank.csv' as a Spark dataframe and name it as df\n",
    "df = pd.read_csv('bank.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wKQMhrtFChHa"
   },
   "source": [
    "## 3. Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VU5xMqN_RyM2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age     job  marital  education default  balance housing loan  contact  \\\n",
      "0   59  admin.  married  secondary      no     2343     yes   no  unknown   \n",
      "1   56  admin.  married  secondary      no       45      no   no  unknown   \n",
      "\n",
      "   day month  duration  campaign  pdays  previous poutcome deposit  \n",
      "0    5   may      1042         1     -1         0  unknown     yes  \n",
      "1    5   may      1467         1     -1         0  unknown     yes  \n"
     ]
    }
   ],
   "source": [
    "# select features ('age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'campaign', 'pdays', 'previous', 'poutcome', 'deposit') as df2\n",
    "df2=df.iloc[[0, 1]]\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vwF5sqRYa_eI"
   },
   "outputs": [],
   "source": [
    "# remove invalid rows/records using spark.sql "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A_T8qvxzR-oI"
   },
   "outputs": [],
   "source": [
    "# convert categorical features to numeric features  using One hot encoding, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pOqHERYJCQuC"
   },
   "source": [
    "### 3.1 normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cfL6_sca5VwI"
   },
   "outputs": [],
   "source": [
    "# then apply Min-Max normalisation on each attribute using MinMaxScaler  \n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9trUY7ZgCzhW"
   },
   "source": [
    "## 4. Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KTQfUch2Cmmi"
   },
   "source": [
    "### 4.1 K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGGZI70Ohqgg"
   },
   "outputs": [],
   "source": [
    "# Perform unsupervised learning on df2 with k-means \n",
    "# you can use whole df2 as both training and testing data, \n",
    "# evaluate the clustering result using Accuracy.  \n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FHom8o2KCt36"
   },
   "source": [
    "### 4.2 PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wT4cx5uGTjmj"
   },
   "outputs": [],
   "source": [
    "# generate a scatter plot using the first two PCA components to investigate the data distribution.\n",
    "\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ulp_uILXCv4Z"
   },
   "source": [
    "## 5. Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0O7tszcPfnHN"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'randomSplit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-b04ce344fde3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandomSplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m742\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training Dataset Count: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test Dataset Count: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5463\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5464\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5465\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5467\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'randomSplit'"
     ]
    }
   ],
   "source": [
    "train, test = df2.randomSplit([0.7, 0.3], seed = 742)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2SsHdh7YC-eN"
   },
   "source": [
    "### 5.1 LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vqo_ywFQYxSj"
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hANwFUzhgG83"
   },
   "outputs": [],
   "source": [
    "# exam the coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "evM5eiJoDHw2"
   },
   "source": [
    "### 5.2 Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "He4mlHb7hBoY"
   },
   "outputs": [],
   "source": [
    "# Decision tree\n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CaE-Z_IlDKXF"
   },
   "source": [
    "### 5.3 NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v2XL6I0t7irt"
   },
   "outputs": [],
   "source": [
    "# NaiveBayes\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SIT742Assignment2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
